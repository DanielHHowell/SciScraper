<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
        <title>Reusable Form Demo</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/paper/bootstrap.min.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/malihu-custom-scrollbar-plugin/3.1.5/jquery.mCustomScrollbar.min.css">
        <link rel= "stylesheet" type= "text/css" href= "{{ url_for('static',filename='styles/zoom.css') }}">
       <meta name="viewport" content = "width=device-width, initial-scale=1.0">

    </head>
    <body>

    <nav class="navbar navbar-inverse">
      <div class="container-fluid">

    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand">SciScraper</a>
    </div>


    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li ><a href="index.html">New Search</a></li>
      </ul>


      </div>
    </div>

    </nav>

<div class="container">


  <h2>SciScraper</h2>
  <form action="" method="post">
    {{ form.csrf }}
    <div class="form-group">
      <label for="topic">Main topic:</label>
      <input type="text" class="form-control" id="topic" name="topic" placeholder="e.g. grid cells">
      <br>
      <label for="queries">Additional queries (comma separated):</label>
      <input type="text" class="form-control" id="queries" name="queries" placeholder="e.g. neurons, brain">
      <br>
      <label for="nResults">Number of Results:</label>
      <input type="text" class="form-control" id="nResults" name="nResults" placeholder="e.g. 10">


    </div>
    <button type="submit" class="btn btn-success">Search</button>
  </form>

<br><br><br>
    <div class="jumbotron">
        <h2>About</h2>
        <p>The goal of this project was to give the searching of NCBI a facelift. Pouring
        through the database is an outdated interaction, which here has been made into a
        single-page experience with downloadable reports available as well
            (in either HTML or PDF). The scraper uses their native API which does not always
            provide complete information, and for now results in occasional formatting errors.
            Other areas of improvement include the slow report generation speed (5-10 seconds) as well as
            an inherent difficulty finding the most relevant images and keywords algorithmically.</p>
        <p><a href="https://github.com/DanielHHowell/SciScraper">Link to GitHub page</a></p>
        <p>Send suggestions or feature ideas here~</p>
        <p>- Daniel Howell</p>
    </div>

<br>

<h3>Example report below:</h3>
<br>

    <h3 style="color:#000099"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5388540" target="_blank"><i>Emergence Of Transformation-Tolerant Representations Of Visual Objects In Rat Lateral Extrastriate Cortex </i></a></h3>
    <br>
    <ul>
        <li>PMC: 5388540 </li>
        <li>DOI: 10.7554/eLife.22794 </li>
        <li>2017 Apr 11 </li>
        <li>[&#39;Tafazoli S&#39;, &#39;Safaai H&#39;, &#39;De Franceschi G&#39;, &#39;Rosselli FB&#39;, &#39;Vanzella W&#39;, &#39;Riggi M&#39;, &#39;Buffolo F&#39;, &#39;Panzeri S&#39;, &#39;Zoccolan D&#39;] </li>
    </ul>
    <br>
    <p style="line-height:2.5;font-size:15px">Rodents are emerging as increasingly popular models of visual functions. Yet, evidence that rodent visual cortex is capable of advanced visual processing, such as object recognition, is limited. Here we investigate how neurons located along the progression of extrastriate areas that, in the rat brain, run laterally to primary visual cortex, encode object information. We found a progressive functional specialization of neural responses along these areas, with: (1) a sharp reduction of the amount of low-level, energy-related visual information encoded by neuronal firing; and (2) a substantial increase in the ability of both single neurons and neuronal populations to support discrimination of visual objects under identity-preserving transformations (e.g., position and size changes). These findings strongly argue for the existence of a rat object-processing pathway, and point to the rodents as promising models to dissect the neuronal circuitry underlying transformation-tolerant recognition of visual objects.</p>

    <br>
          <div class="container">
    <p>Figures from article (hover to zoom):</p>
    <div class="row">
        <ul class="list-inline gallery">

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5388540/bin/elife-22794-fig2.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5388540/bin/elife-22794-fig3.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5388540/bin/elife-22794-fig4.jpg" alt=""></div>

        </ul>

    </div>
          </div>
    <br>
    <br>
    <hr>
    <hr>
    <br>

    <h3 style="color:#000099"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444" target="_blank"><i>How Does The Brain Solve Visual Object Recognition? </i></a></h3>
    <br>
    <ul>
        <li>PMC: 3306444 </li>
        <li>DOI: 10.1016/j.neuron.2012.01.010 </li>
        <li>2012 Feb 9 </li>
        <li>[&#39;DiCarlo JJ&#39;, &#39;Zoccolan D&#39;, &#39;Rust NC&#39;] </li>
    </ul>
    <br>
    <p style="line-height:2.5;font-size:15px">Mounting evidence suggests that “core object recognition,” the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains little-understood. Here we review evidence ranging from individual neurons, to neuronal populations, to behavior, to computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical sub-networks with a common functional goal.</p>

    <br>
          <div class="container">
    <p>Figures from article (hover to zoom):</p>
    <div class="row">
        <ul class="list-inline gallery">

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC3306444/bin/nihms352068f2.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC3306444/bin/nihms352068f3.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC3306444/bin/nihms352068f4.jpg" alt=""></div>

        </ul>

    </div>
          </div>
    <br>
    <br>
    <hr>
    <hr>
    <br>

    <h3 style="color:#000099"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4572081" target="_blank"><i>Invariant Visual Object Recognition: Biologically Plausible Approaches </i></a></h3>
    <br>
    <ul>
        <li>PMC: 4572081 </li>
        <li>DOI: 10.1007/s00422-015-0658-2 </li>
        <li>2015 Sep 3 </li>
        <li>[&#39;Robinson L&#39;, &#39;Rolls ET&#39;] </li>
    </ul>
    <br>
    <p style="line-height:2.5;font-size:15px">Key properties of inferior temporal cortex neurons are described, and then, the biological plausibility of two leading approaches to invariant visual object recognition in the ventral visual system is assessed to investigate whether they account for these properties. Experiment 1 shows that VisNet performs object classification with random exemplars comparably to HMAX, except that the final layer C neurons of HMAX have a very non-sparse representation (unlike that in the brain) that provides little information in the single-neuron responses about the object class. Experiment 2 shows that VisNet forms invariant representations when trained with different views of each object, whereas HMAX performs poorly when assessed with a biologically plausible pattern association network, as HMAX has no mechanism to learn view invariance. Experiment 3 shows that VisNet neurons do not respond to scrambled images of faces, and thus encode shape information. HMAX neurons responded with similarly high rates to the unscrambled and scrambled faces, indicating that low-level features including texture may be relevant to HMAX performance. Experiment 4 shows that VisNet can learn to recognize objects even when the view provided by the object changes catastrophically as it transforms, whereas HMAX has no learning mechanism in its S–C hierarchy that provides for view-invariant learning. This highlights some requirements for the neurobiological mechanisms of high-level vision, and how some different approaches perform, in order to help understand the fundamental underlying principles of invariant visual object recognition in the ventral visual stream.</p>

    <br>
          <div class="container">
    <p>Figures from article (hover to zoom):</p>
    <div class="row">
        <ul class="list-inline gallery">

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4572081/table/Tab1/?report=previmg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4572081/table/Tab3/?report=previmg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4572081/table/Tab2/?report=previmg" alt=""></div>

        </ul>

    </div>
          </div>
    <br>
    <br>
    <hr>
    <hr>
    <br>

    <h3 style="color:#000099"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4491543" target="_blank"><i>Neuronal Reward And Decision Signals: From Theories To Data </i></a></h3>
    <br>
    <ul>
        <li>PMC: 4491543 </li>
        <li>DOI: 10.1152/physrev.00023.2014 </li>
        <li>2015 Jun 24 </li>
        <li>[&#39;Schultz W&#39;] </li>
    </ul>
    <br>
    <p style="line-height:2.5;font-size:15px">Rewards are crucial objects that induce learning, approach behavior, choices, and emotions. Whereas emotions are difficult to investigate in animals, the learning function is mediated by neuronal reward prediction error signals which implement basic constructs of reinforcement learning theory. These signals are found in dopamine neurons, which emit a global reward signal to striatum and frontal cortex, and in specific neurons in striatum, amygdala, and frontal cortex projecting to select neuronal populations. The approach and choice functions involve subjective value, which is objectively assessed by behavioral choices eliciting internal, subjective reward preferences. Utility is the formal mathematical characterization of subjective value and a prime decision variable in economic choice theory. It is coded as utility prediction error by phasic dopamine responses. Utility can incorporate various influences, including risk, delay, effort, and social interaction. Appropriate for formal decision mechanisms, rewards are coded as object value, action value, difference value, and chosen value by specific neurons. Although all reward, reinforcement, and decision variables are theoretical constructs, their neuronal signals constitute measurable physical implementations and as such confirm the validity of these concepts. The neuronal reward signals provide guidance for behavior while constraining the free will to act.</p>

    <br>
          <div class="container">
    <p>Figures from article (hover to zoom):</p>
    <div class="row">
        <ul class="list-inline gallery">

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4491543/bin/z9j0031527320002.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4491543/bin/z9j0031527320003.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC4491543/bin/z9j0031527320004.jpg" alt=""></div>

        </ul>

    </div>
          </div>
    <br>
    <br>
    <hr>
    <hr>
    <br>

    <div class="jumbotron">

    <h3 style="color:#000099"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5040375" target="_blank"><i>A Simple Approach To Ignoring Irrelevant Variables By Population Decoding Based On Multisensory Neurons </i></a></h3>
    <br>
    <ul>
        <li>PMC: 5040375 </li>
        <li>DOI: 10.1152/jn.00005.2016 </li>
        <li>2016 Jun 22 </li>
        <li>[&#39;Kim HR&#39;, &#39;Pitkow X&#39;, &#39;Angelaki DE&#39;, &#39;DeAngelis GC&#39;] </li>
    </ul>
    <br>
    <p style="line-height:2.5;font-size:15px">Sensory signals often reflect multiple variables that change in the environment. To estimate one task-relevant variable and ignore others, the brain needs to perform marginalization, which involves computing the probability distribution over one variable while averaging over irrelevant variables. We describe a computational approach by which a linear transformation of neural population responses can approximate marginalization. We show, through simulations involving diverse multisensory neurons, that this approach is effective in dissociating self-motion from object motion.</p>

    <br>
          <div class="container">
    <p>Figures from article (hover to zoom):</p>
    <div class="row">
        <ul class="list-inline gallery">

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5040375/bin/z9k0091637870005.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5040375/bin/z9k0091637870002.jpg" alt=""></div>

            <div class="col-lg-4 col-xs-6 thumbnail zoom"><img src="https://www.ncbi.nlm.nih.gov//pmc/articles/PMC5040375/bin/z9k0091637870004.jpg" alt=""></div>

        </ul>

    </div>
          </div>
    <br>
    <br>
    <hr>
    <hr>
    <br>


    </div>
    </div>

</body>
</html>